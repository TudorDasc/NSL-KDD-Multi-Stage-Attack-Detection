{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab04307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52e34abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-STAGE ATTACK DETECTION PIPELINE\n",
      "NSL-KDD with HistGradientBoosting + Corrected Temporal Features\n",
      "================================================================================\n",
      "\n",
      "Loading NSL-KDD dataset...\n",
      "Train samples: 125973, Test samples: 22544\n",
      "\n",
      "Extracting temporal features...\n",
      "\n",
      "================================================================================\n",
      "PHASE 1: TRAIN BASELINE MODEL\n",
      "================================================================================\n",
      "Training completed in 5.40s\n",
      "\n",
      "================================================================================\n",
      "PHASE 2: TRAIN TEMPORAL MODEL\n",
      "================================================================================\n",
      "Training completed in 6.47s\n",
      "\n",
      "================================================================================\n",
      "PHASE 3: EVALUATION (TEMPORAL MODEL)\n",
      "================================================================================\n",
      "================================================================================\n",
      "MULTI-CLASS ATTACK DETECTION RESULTS\n",
      "================================================================================\n",
      "Overall Accuracy (valid labels): 0.8097\n",
      "Weighted F1 (valid labels):      0.7880\n",
      "Macro F1 (valid labels):         0.5989\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal     0.7384    0.9717    0.8391      9711\n",
      "         DoS     0.9644    0.8174    0.8848      7458\n",
      "       Probe     0.7726    0.8406    0.8051      2421\n",
      "         R2L     0.8514    0.2456    0.3813      2752\n",
      "         U2R     0.6429    0.0450    0.0841       200\n",
      "\n",
      "    accuracy                         0.8097     22542\n",
      "   macro avg     0.7939    0.5841    0.5989     22542\n",
      "weighted avg     0.8298    0.8097    0.7880     22542\n",
      "\n",
      "Confusion Matrix (valid labels):\n",
      "        Normal   DoS  Probe  R2L  U2R\n",
      "Normal    9436    54    212    7    2\n",
      "DoS       1076  6096    262   24    0\n",
      "Probe      134   171   2035   81    0\n",
      "R2L       2057     0     16  676    3\n",
      "U2R         76     0    109    6    9\n",
      "\n",
      "Performance Comparison (valid labels only):\n",
      "                 Model  Accuracy  Precision   Recall  F1-Score\n",
      "              Baseline  0.796735   0.807129 0.796735  0.769408\n",
      "With Temporal Features  0.809689   0.829791 0.809689  0.788002\n",
      "\n",
      "================================================================================\n",
      "PHASE 4: TEMPORAL FEATURE IMPORTANCE (PERMUTATION)\n",
      "================================================================================\n",
      "Top 10 temporal features (permutation importance):\n",
      "              feature  importance\n",
      "failed_login_rate_100    0.015113\n",
      "   total_src_bytes_30    0.012417\n",
      "       avg_duration_5    0.011401\n",
      "    total_src_bytes_2    0.008401\n",
      "     avg_duration_100    0.007043\n",
      "  total_src_bytes_100    0.004725\n",
      "   total_src_bytes_10    0.002605\n",
      "    total_dst_bytes_2    0.001803\n",
      " failed_login_rate_30    0.001294\n",
      "   total_dst_bytes_10    0.001027\n",
      "\n",
      "================================================================================\n",
      "PHASE 5: MULTI-STAGE ATTACK SEQUENCES\n",
      "================================================================================\n",
      "Detected sequences: 2180\n",
      "\n",
      "================================================================================\n",
      "PHASE 6: MITRE ATT&CK MAPPING\n",
      "================================================================================\n",
      "Unique TTPs detected: 15\n",
      "\n",
      "================================================================================\n",
      "EXPORT COMPLETE\n",
      "================================================================================\n",
      "Saved:\n",
      "  baseline_vs_temporal_comparison.csv\n",
      "  temporal_feature_importance.csv (if computed)\n",
      "  dashboard_data.json\n",
      "\n",
      "Open your HTML dashboard via http.server and load the page in the browser\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MITRE ATT&CK Mapping for NSL-KDD Attack Categories\n",
    "# ============================================================================\n",
    "ATTACK_TO_MITRE = {\n",
    "    # DoS\n",
    "    \"back\": [\"T1499.002\"],\n",
    "    \"land\": [\"T1499.004\"],\n",
    "    \"neptune\": [\"T1498.001\"],\n",
    "    \"pod\": [\"T1499.004\"],\n",
    "    \"smurf\": [\"T1498.001\"],\n",
    "    \"teardrop\": [\"T1499.004\"],\n",
    "    \"apache2\": [\"T1499.002\"],\n",
    "    \"udpstorm\": [\"T1498.001\"],\n",
    "    \"processtable\": [\"T1499.003\"],\n",
    "    \"mailbomb\": [\"T1499.002\"],\n",
    "    # Probe\n",
    "    \"ipsweep\": [\"T1046\"],\n",
    "    \"nmap\": [\"T1046\"],\n",
    "    \"portsweep\": [\"T1046\"],\n",
    "    \"satan\": [\"T1046\"],\n",
    "    \"mscan\": [\"T1046\"],\n",
    "    \"saint\": [\"T1046\"],\n",
    "    # R2L\n",
    "    \"ftp_write\": [\"T1071.002\"],\n",
    "    \"guess_passwd\": [\"T1110.001\"],\n",
    "    \"imap\": [\"T1078\"],\n",
    "    \"multihop\": [\"T1090\"],\n",
    "    \"phf\": [\"T1190\"],\n",
    "    \"spy\": [\"T1056.001\"],\n",
    "    \"warezclient\": [\"T1071.001\"],\n",
    "    \"warezmaster\": [\"T1071.001\"],\n",
    "    \"sendmail\": [\"T1190\"],\n",
    "    \"named\": [\"T1190\"],\n",
    "    \"snmpgetattack\": [\"T1046\"],\n",
    "    \"snmpguess\": [\"T1110.001\"],\n",
    "    \"xlock\": [\"T1110.001\"],\n",
    "    \"xsnoop\": [\"T1056.001\"],\n",
    "    # U2R\n",
    "    \"buffer_overflow\": [\"T1068\"],\n",
    "    \"loadmodule\": [\"T1068\"],\n",
    "    \"perl\": [\"T1059.006\"],\n",
    "    \"rootkit\": [\"T1014\"],\n",
    "    \"ps\": [\"T1057\"],\n",
    "    \"sqlattack\": [\"T1190\"],\n",
    "    \"xterm\": [\"T1068\"],\n",
    "    \"httptunnel\": [\"T1572\"],\n",
    "}\n",
    "\n",
    "ATTACK_STAGES = {\n",
    "    \"Reconnaissance\": [\"ipsweep\", \"nmap\", \"portsweep\", \"satan\", \"mscan\", \"saint\"],\n",
    "    \"Initial Access\": [\"ftp_write\", \"phf\", \"sendmail\", \"named\", \"snmpgetattack\", \"sqlattack\"],\n",
    "    \"Credential Access\": [\"guess_passwd\", \"snmpguess\", \"xlock\"],\n",
    "    \"Privilege Escalation\": [\"buffer_overflow\", \"loadmodule\", \"perl\", \"rootkit\", \"ps\", \"xterm\", \"httptunnel\"],\n",
    "    \"Persistence\": [\"spy\", \"xsnoop\", \"rootkit\"],\n",
    "    \"Lateral Movement\": [\"multihop\", \"warezmaster\"],\n",
    "    \"Impact\": [\"back\", \"land\", \"neptune\", \"pod\", \"smurf\", \"teardrop\", \"apache2\", \"udpstorm\", \"processtable\", \"mailbomb\"],\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Data loading and labels\n",
    "# ============================================================================\n",
    "def load_nsl_kdd(trainPath=\"./dataset/KDDTrain+.txt\", testPath=\"./dataset/KDDTest+.txt\"):\n",
    "    \"\"\"Load NSL-KDD dataset with all columns\"\"\"\n",
    "    columns = [\n",
    "        \"duration\",\n",
    "        \"protocol_type\",\n",
    "        \"service\",\n",
    "        \"flag\",\n",
    "        \"src_bytes\",\n",
    "        \"dst_bytes\",\n",
    "        \"land\",\n",
    "        \"wrong_fragment\",\n",
    "        \"urgent\",\n",
    "        \"hot\",\n",
    "        \"num_failed_logins\",\n",
    "        \"logged_in\",\n",
    "        \"num_compromised\",\n",
    "        \"root_shell\",\n",
    "        \"su_attempted\",\n",
    "        \"num_root\",\n",
    "        \"num_file_creations\",\n",
    "        \"num_shells\",\n",
    "        \"num_access_files\",\n",
    "        \"num_outbound_cmds\",\n",
    "        \"is_host_login\",\n",
    "        \"is_guest_login\",\n",
    "        \"count\",\n",
    "        \"srv_count\",\n",
    "        \"serror_rate\",\n",
    "        \"srv_serror_rate\",\n",
    "        \"rerror_rate\",\n",
    "        \"srv_rerror_rate\",\n",
    "        \"same_srv_rate\",\n",
    "        \"diff_srv_rate\",\n",
    "        \"srv_diff_host_rate\",\n",
    "        \"dst_host_count\",\n",
    "        \"dst_host_srv_count\",\n",
    "        \"dst_host_same_srv_rate\",\n",
    "        \"dst_host_diff_srv_rate\",\n",
    "        \"dst_host_same_src_port_rate\",\n",
    "        \"dst_host_srv_diff_host_rate\",\n",
    "        \"dst_host_serror_rate\",\n",
    "        \"dst_host_srv_serror_rate\",\n",
    "        \"dst_host_rerror_rate\",\n",
    "        \"dst_host_srv_rerror_rate\",\n",
    "        \"label\",\n",
    "        \"difficulty\",\n",
    "    ]\n",
    "    trainDf = pd.read_csv(trainPath, names=columns)\n",
    "    testDf = pd.read_csv(testPath, names=columns)\n",
    "    return trainDf, testDf\n",
    "\n",
    "\n",
    "def create_attack_category_labels(df):\n",
    "    \"\"\"\n",
    "    Create multi-class labels based on NSL-KDD categories\n",
    "    0: Normal, 1: DoS, 2: Probe, 3: R2L, 4: U2R\n",
    "    Unknown attacks are mapped to -1 so they can be excluded from evaluation\n",
    "    \"\"\"\n",
    "    labelSeries = df[\"label\"].astype(str).str.strip().str.rstrip(\".\")\n",
    "\n",
    "    dos = {\n",
    "        \"back\",\n",
    "        \"land\",\n",
    "        \"neptune\",\n",
    "        \"pod\",\n",
    "        \"smurf\",\n",
    "        \"teardrop\",\n",
    "        \"apache2\",\n",
    "        \"udpstorm\",\n",
    "        \"processtable\",\n",
    "        \"mailbomb\",\n",
    "    }\n",
    "    probe = {\"ipsweep\", \"nmap\", \"portsweep\", \"satan\", \"mscan\", \"saint\"}\n",
    "    r2l = {\n",
    "        \"ftp_write\",\n",
    "        \"guess_passwd\",\n",
    "        \"imap\",\n",
    "        \"multihop\",\n",
    "        \"phf\",\n",
    "        \"spy\",\n",
    "        \"warezclient\",\n",
    "        \"warezmaster\",\n",
    "        \"sendmail\",\n",
    "        \"named\",\n",
    "        \"snmpgetattack\",\n",
    "        \"snmpguess\",\n",
    "        \"xlock\",\n",
    "        \"xsnoop\",\n",
    "    }\n",
    "    u2r = {\"buffer_overflow\", \"loadmodule\", \"perl\", \"rootkit\", \"ps\", \"sqlattack\", \"xterm\", \"httptunnel\"}\n",
    "\n",
    "    attackToCategory = {\"normal\": 0}\n",
    "    attackToCategory.update({a: 1 for a in dos})\n",
    "    attackToCategory.update({a: 2 for a in probe})\n",
    "    attackToCategory.update({a: 3 for a in r2l})\n",
    "    attackToCategory.update({a: 4 for a in u2r})\n",
    "\n",
    "    y = labelSeries.map(attackToCategory).fillna(-1).astype(int).to_numpy()\n",
    "    return y\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Temporal features (corrected: rolling is per connGroup, not global)\n",
    "# ============================================================================\n",
    "def extract_temporal_features(df, windowSizes=(2, 5, 10, 30, 100)):\n",
    "    \"\"\"Extract temporal features using per-connGroup rolling statistics\"\"\"\n",
    "    dfCopy = df.copy()\n",
    "\n",
    "    connGroup = (\n",
    "        dfCopy[\"protocol_type\"].astype(str)\n",
    "        + \"_\"\n",
    "        + dfCopy[\"service\"].astype(str)\n",
    "        + \"_\"\n",
    "        + dfCopy[\"flag\"].astype(str)\n",
    "    )\n",
    "\n",
    "    grouped = dfCopy.groupby(connGroup, sort=False)\n",
    "    temporal = pd.DataFrame(index=dfCopy.index)\n",
    "\n",
    "    for window in windowSizes:\n",
    "        temporal[f\"conn_count_{window}\"] = grouped[\"duration\"].transform(\n",
    "            lambda s: s.rolling(window=window, min_periods=1).count()\n",
    "        )\n",
    "        temporal[f\"avg_duration_{window}\"] = grouped[\"duration\"].transform(\n",
    "            lambda s: s.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        temporal[f\"total_src_bytes_{window}\"] = grouped[\"src_bytes\"].transform(\n",
    "            lambda s: s.rolling(window=window, min_periods=1).sum()\n",
    "        )\n",
    "        temporal[f\"total_dst_bytes_{window}\"] = grouped[\"dst_bytes\"].transform(\n",
    "            lambda s: s.rolling(window=window, min_periods=1).sum()\n",
    "        )\n",
    "        temporal[f\"failed_login_rate_{window}\"] = grouped[\"num_failed_logins\"].transform(\n",
    "            lambda s: s.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        temporal[f\"serror_rate_avg_{window}\"] = grouped[\"serror_rate\"].transform(\n",
    "            lambda s: s.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "\n",
    "    temporal[\"protocol_change\"] = (dfCopy[\"protocol_type\"] != dfCopy[\"protocol_type\"].shift(1)).astype(int)\n",
    "    temporal[\"service_change\"] = (dfCopy[\"service\"] != dfCopy[\"service\"].shift(1)).astype(int)\n",
    "    temporal[\"flag_change\"] = (dfCopy[\"flag\"] != dfCopy[\"flag\"].shift(1)).astype(int)\n",
    "\n",
    "    # Vectorized time since last failed login (global sequence)\n",
    "    failedMask = (dfCopy[\"num_failed_logins\"].to_numpy() > 0).astype(int)\n",
    "    idxArr = np.arange(len(dfCopy), dtype=int)\n",
    "    lastFailedIdx = np.where(failedMask == 1, idxArr, -1)\n",
    "    lastFailedIdx = pd.Series(lastFailedIdx).replace(-1, np.nan).ffill().fillna(-1).to_numpy(dtype=int)\n",
    "    timeSince = np.where(lastFailedIdx == -1, 999, np.clip(idxArr - lastFailedIdx, 0, 999))\n",
    "    temporal[\"time_since_failed_login\"] = timeSince\n",
    "\n",
    "    # Burst and spike indicators (based on within-group rolling signals already created)\n",
    "    if \"conn_count_10\" in temporal.columns:\n",
    "        prevConn = temporal[\"conn_count_10\"].shift(10).fillna(0)\n",
    "        temporal[\"conn_burst\"] = (temporal[\"conn_count_10\"] > (prevConn * 2)).astype(int)\n",
    "    else:\n",
    "        temporal[\"conn_burst\"] = 0\n",
    "\n",
    "    if \"total_src_bytes_10\" in temporal.columns and \"total_dst_bytes_10\" in temporal.columns:\n",
    "        trafficNow = temporal[\"total_src_bytes_10\"] + temporal[\"total_dst_bytes_10\"]\n",
    "        trafficPrev = (temporal[\"total_src_bytes_10\"].shift(10).fillna(0) +\n",
    "                       temporal[\"total_dst_bytes_10\"].shift(10).fillna(0))\n",
    "        temporal[\"traffic_spike\"] = (trafficNow > (trafficPrev * 3)).astype(int)\n",
    "    else:\n",
    "        temporal[\"traffic_spike\"] = 0\n",
    "\n",
    "    temporal = temporal.fillna(0)\n",
    "    return temporal\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Preprocessing and model\n",
    "# ============================================================================\n",
    "def build_preprocessor(trainX):\n",
    "    \"\"\"Build preprocessing pipeline (dense output for HistGradientBoosting)\"\"\"\n",
    "    categoricalCols = [\"protocol_type\", \"service\", \"flag\"]\n",
    "    numericCols = [c for c in trainX.columns if c not in categoricalCols]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categoricalCols),\n",
    "            (\"num\", \"passthrough\", numericCols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "def train_multiclass_model(trainX, trainY):\n",
    "    \"\"\"Train HistGradientBoosting with balanced sample weights\"\"\"\n",
    "    preprocessor = build_preprocessor(trainX)\n",
    "\n",
    "    model = HistGradientBoostingClassifier(\n",
    "        max_iter=400,\n",
    "        learning_rate=0.05,\n",
    "        max_leaf_nodes=63,\n",
    "        min_samples_leaf=20,\n",
    "        l2_regularization=0.1,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline([(\"preprocess\", preprocessor), (\"model\", model)])\n",
    "\n",
    "    sampleWeight = compute_sample_weight(class_weight=\"balanced\", y=trainY)\n",
    "    startTime = time.time()\n",
    "    pipeline.fit(trainX, trainY, model__sample_weight=sampleWeight)\n",
    "    trainTime = time.time() - startTime\n",
    "\n",
    "    print(f\"Training completed in {trainTime:.2f}s\")\n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def evaluate_multiclass(pipeline, testX, testY, validMask=None):\n",
    "    \"\"\"Evaluate multi-class model performance (optionally ignoring unknown labels -1)\"\"\"\n",
    "    predY = pipeline.predict(testX)\n",
    "\n",
    "    if validMask is None:\n",
    "        validMask = np.ones_like(testY, dtype=bool)\n",
    "\n",
    "    testYValid = testY[validMask]\n",
    "    predYValid = predY[validMask]\n",
    "\n",
    "    classNames = [\"Normal\", \"DoS\", \"Probe\", \"R2L\", \"U2R\"]\n",
    "\n",
    "    acc = accuracy_score(testYValid, predYValid)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        testYValid, predYValid, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    macroF1 = precision_recall_fscore_support(\n",
    "        testYValid, predYValid, average=\"macro\", zero_division=0\n",
    "    )[2]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MULTI-CLASS ATTACK DETECTION RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Overall Accuracy (valid labels): {acc:.4f}\")\n",
    "    print(f\"Weighted F1 (valid labels):      {f1:.4f}\")\n",
    "    print(f\"Macro F1 (valid labels):         {macroF1:.4f}\")\n",
    "    print()\n",
    "    print(classification_report(testYValid, predYValid, target_names=classNames, digits=4, zero_division=0))\n",
    "\n",
    "    cm = confusion_matrix(testYValid, predYValid, labels=[0, 1, 2, 3, 4])\n",
    "    cmDf = pd.DataFrame(cm, index=classNames, columns=classNames)\n",
    "    print(\"Confusion Matrix (valid labels):\")\n",
    "    print(cmDf)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision\": float(prec),\n",
    "        \"recall\": float(rec),\n",
    "        \"f1score\": float(f1),\n",
    "        \"macroF1\": float(macroF1),\n",
    "    }\n",
    "    return predY, metrics\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Analysis helpers\n",
    "# ============================================================================\n",
    "def detect_attack_sequences(dfWithTemporal, predictions, windowSize=20):\n",
    "    \"\"\"Detect multi-stage attack patterns and export explicit severity reasoning\"\"\"\n",
    "    classNames = [\"Normal\", \"DoS\", \"Probe\", \"R2L\", \"U2R\"]\n",
    "    attackSequences = []\n",
    "\n",
    "    stepSize = max(1, windowSize // 2)\n",
    "    n = len(predictions)\n",
    "\n",
    "    # Tunable thresholds for explainability\n",
    "    burstHighThreshold = 8\n",
    "    spikeHighThreshold = 8\n",
    "\n",
    "    for startIdx in range(0, n - windowSize + 1, stepSize):\n",
    "        endIdx = startIdx + windowSize\n",
    "        window = predictions[startIdx:endIdx]\n",
    "\n",
    "        # Count classes in the window\n",
    "        counts = np.bincount(window, minlength=5)  # indices 0..4\n",
    "        attackTypeCounts = {\n",
    "            \"Normal\": int(counts[0]),\n",
    "            \"DoS\": int(counts[1]),\n",
    "            \"Probe\": int(counts[2]),\n",
    "            \"R2L\": int(counts[3]),\n",
    "            \"U2R\": int(counts[4]),\n",
    "        }\n",
    "\n",
    "        uniqueAttacks = set(np.unique(window[window > 0]))\n",
    "        if len(uniqueAttacks) < 2:\n",
    "            continue\n",
    "\n",
    "        # Temporal indicators (keep original fields)\n",
    "        temporalIndicators = {}\n",
    "        burstCount = 0\n",
    "        spikeCount = 0\n",
    "\n",
    "        if \"conn_burst\" in dfWithTemporal.columns:\n",
    "            burstCount = int(dfWithTemporal.iloc[startIdx:endIdx][\"conn_burst\"].sum())\n",
    "            temporalIndicators[\"burst_count\"] = burstCount\n",
    "\n",
    "        if \"traffic_spike\" in dfWithTemporal.columns:\n",
    "            spikeCount = int(dfWithTemporal.iloc[startIdx:endIdx][\"traffic_spike\"].sum())\n",
    "            temporalIndicators[\"traffic_spikes\"] = spikeCount\n",
    "\n",
    "        # Base outputs\n",
    "        severity = \"MEDIUM\"\n",
    "        pattern = \"Multi-vector Attack\"\n",
    "\n",
    "        # Pattern logic (same as before, but we also capture order evidence)\n",
    "        attackOrder = [int(p) for p in window if p > 0]\n",
    "        orderEvidence = {}\n",
    "\n",
    "        hasDoS = 1 in uniqueAttacks\n",
    "        hasProbe = 2 in uniqueAttacks\n",
    "        hasR2L = 3 in uniqueAttacks\n",
    "        hasU2R = 4 in uniqueAttacks\n",
    "\n",
    "        if hasProbe and (hasR2L or hasU2R):\n",
    "            probeIdx = next((i for i, a in enumerate(attackOrder) if a == 2), None)\n",
    "            exploitIdx = next((i for i, a in enumerate(attackOrder) if a in [3, 4]), None)\n",
    "            orderEvidence = {\"probe_idx\": probeIdx, \"exploit_idx\": exploitIdx}\n",
    "\n",
    "            if probeIdx is not None and exploitIdx is not None and probeIdx < exploitIdx:\n",
    "                pattern = \"Reconnaissance -> Exploitation (Kill Chain)\"\n",
    "                severity = \"CRITICAL\"\n",
    "            else:\n",
    "                pattern = \"Reconnaissance + Exploitation (Parallel)\"\n",
    "                severity = \"CRITICAL\"\n",
    "        elif hasR2L and hasU2R:\n",
    "            pattern = \"Initial Access -> Privilege Escalation\"\n",
    "            severity = \"CRITICAL\"\n",
    "        elif hasDoS and len(uniqueAttacks) >= 2:\n",
    "            pattern = \"DoS + Other Attack (Distraction Tactic)\"\n",
    "            severity = \"HIGH\"\n",
    "        elif hasU2R:\n",
    "            # If U2R appears with something else, treat as high even if no explicit chain detected\n",
    "            severity = \"HIGH\"\n",
    "\n",
    "        # Explainability: compute a score + reasons\n",
    "        reasonCodes = []\n",
    "        reasonText = []\n",
    "        score = 0\n",
    "\n",
    "        # Pattern-based weight\n",
    "        if \"Kill Chain\" in pattern:\n",
    "            score += 55\n",
    "            reasonCodes.append(\"KILL_CHAIN_ORDERED\")\n",
    "            reasonText.append(\"Probe detected before exploitation in the same window (kill-chain progression)\")\n",
    "\n",
    "        if \"Parallel\" in pattern:\n",
    "            score += 45\n",
    "            reasonCodes.append(\"RECON_EXPLOIT_PARALLEL\")\n",
    "            reasonText.append(\"Reconnaissance and exploitation co-occur in the same window\")\n",
    "\n",
    "        if \"Privilege Escalation\" in pattern:\n",
    "            score += 55\n",
    "            reasonCodes.append(\"ACCESS_TO_ROOT_CHAIN\")\n",
    "            reasonText.append(\"R2L and U2R both present (access followed by privilege escalation)\")\n",
    "\n",
    "        if \"Distraction Tactic\" in pattern:\n",
    "            score += 25\n",
    "            reasonCodes.append(\"DOS_DISTRACTION\")\n",
    "            reasonText.append(\"DoS occurs alongside other attack types (possible distraction)\")\n",
    "\n",
    "        # Presence-based weight\n",
    "        if hasU2R:\n",
    "            score += 35\n",
    "            reasonCodes.append(\"HAS_U2R\")\n",
    "            reasonText.append(\"U2R present (privilege escalation / high impact)\")\n",
    "\n",
    "        if hasR2L:\n",
    "            score += 20\n",
    "            reasonCodes.append(\"HAS_R2L\")\n",
    "            reasonText.append(\"R2L present (remote-to-local compromise)\")\n",
    "\n",
    "        if hasProbe:\n",
    "            score += 10\n",
    "            reasonCodes.append(\"HAS_PROBE\")\n",
    "            reasonText.append(\"Probe present (reconnaissance / scanning)\")\n",
    "\n",
    "        # Volume/temporal-based weight\n",
    "        if burstCount >= burstHighThreshold:\n",
    "            score += 10\n",
    "            reasonCodes.append(\"HIGH_BURST\")\n",
    "            reasonText.append(f\"High connection burst activity (burst_count={burstCount})\")\n",
    "        elif burstCount > 0:\n",
    "            reasonCodes.append(\"BURST_PRESENT\")\n",
    "            reasonText.append(f\"Connection burst activity present (burst_count={burstCount})\")\n",
    "\n",
    "        if spikeCount >= spikeHighThreshold:\n",
    "            score += 10\n",
    "            reasonCodes.append(\"HIGH_TRAFFIC_SPIKE\")\n",
    "            reasonText.append(f\"High traffic spike activity (traffic_spikes={spikeCount})\")\n",
    "        elif spikeCount > 0:\n",
    "            reasonCodes.append(\"TRAFFIC_SPIKE_PRESENT\")\n",
    "            reasonText.append(f\"Traffic spikes present (traffic_spikes={spikeCount})\")\n",
    "\n",
    "        # Cap score\n",
    "        score = int(max(0, min(100, score)))\n",
    "\n",
    "        # Optional: make severity consistent with score (so label always matches numeric evidence)\n",
    "        # If you prefer to keep your old severity logic, comment this block out.\n",
    "        if score >= 70:\n",
    "            severity = \"CRITICAL\"\n",
    "        elif score >= 45:\n",
    "            severity = \"HIGH\"\n",
    "        else:\n",
    "            severity = \"MEDIUM\"\n",
    "\n",
    "        attackSequences.append(\n",
    "            {\n",
    "                \"window_start\": int(startIdx),\n",
    "                \"window_end\": int(endIdx),\n",
    "                \"attack_types\": [classNames[a] for a in sorted(uniqueAttacks)],\n",
    "                \"attack_type_counts\": attackTypeCounts,\n",
    "                \"severity\": severity,\n",
    "                \"severity_score\": score,\n",
    "                \"pattern\": pattern,\n",
    "                \"reason_codes\": reasonCodes,\n",
    "                \"reason_text\": reasonText,\n",
    "                \"order_evidence\": orderEvidence,\n",
    "                \"temporal_indicators\": temporalIndicators,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return attackSequences\n",
    "\n",
    "\n",
    "\n",
    "def map_to_mitre_attack(df, predictions):\n",
    "    \"\"\"Map detected attacks to MITRE ATT&CK techniques and kill chain stages\"\"\"\n",
    "    attackLabels = df[\"label\"].astype(str).str.strip().str.rstrip(\".\").to_numpy()\n",
    "\n",
    "    ttpCounts = defaultdict(int)\n",
    "    stageCounts = defaultdict(int)\n",
    "\n",
    "    for label, pred in zip(attackLabels, predictions):\n",
    "        if pred <= 0:\n",
    "            continue\n",
    "        if label in ATTACK_TO_MITRE:\n",
    "            for ttp in ATTACK_TO_MITRE[label]:\n",
    "                ttpCounts[ttp] += 1\n",
    "            for stage, attacks in ATTACK_STAGES.items():\n",
    "                if label in attacks:\n",
    "                    stageCounts[stage] += 1\n",
    "\n",
    "    return ttpCounts, stageCounts\n",
    "\n",
    "\n",
    "def compute_temporal_feature_importance(pipeline, testX, testY, temporalCols, validMask, maxSamples=5000):\n",
    "    \"\"\"\n",
    "    Compute permutation importance for temporal features only\n",
    "    Returns DataFrame with columns: feature, importance\n",
    "    \"\"\"\n",
    "    if len(temporalCols) == 0:\n",
    "        return None\n",
    "\n",
    "    validIdx = np.where(validMask)[0]\n",
    "    if len(validIdx) == 0:\n",
    "        return None\n",
    "\n",
    "    rng = np.random.default_rng(42)\n",
    "    takeN = min(maxSamples, len(validIdx))\n",
    "    sampleIdx = rng.choice(validIdx, size=takeN, replace=False)\n",
    "\n",
    "    Xsub = testX.iloc[sampleIdx].copy()\n",
    "    ysub = testY[sampleIdx].copy()\n",
    "\n",
    "    # Compute permutation importance over input columns\n",
    "    result = permutation_importance(\n",
    "        pipeline,\n",
    "        Xsub,\n",
    "        ysub,\n",
    "        scoring=\"f1_weighted\",\n",
    "        n_repeats=5,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    featureNames = np.array(Xsub.columns)\n",
    "    importancesMean = result.importances_mean\n",
    "    importanceDf = pd.DataFrame({\"feature\": featureNames, \"importance\": importancesMean})\n",
    "    temporalMask = importanceDf[\"feature\"].isin(temporalCols)\n",
    "    temporalImportanceDf = importanceDf[temporalMask].sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return temporalImportanceDf\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Dashboard export\n",
    "# ============================================================================\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return super().default(obj)\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"MULTI-STAGE ATTACK DETECTION PIPELINE\")\n",
    "print(\"NSL-KDD with HistGradientBoosting + Corrected Temporal Features\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Loading NSL-KDD dataset...\")\n",
    "trainDf, testDf = load_nsl_kdd()\n",
    "print(f\"Train samples: {len(trainDf)}, Test samples: {len(testDf)}\")\n",
    "print()\n",
    "\n",
    "trainYAll = create_attack_category_labels(trainDf)\n",
    "testYAll = create_attack_category_labels(testDf)\n",
    "\n",
    "# Filter unknown labels (-1) from training if any appear\n",
    "trainValidMask = trainYAll != -1\n",
    "trainDf = trainDf.loc[trainValidMask].reset_index(drop=True)\n",
    "trainY = trainYAll[trainValidMask]\n",
    "\n",
    "# Baseline features\n",
    "trainXBase = trainDf.drop(columns=[\"label\", \"difficulty\"])\n",
    "testXBase = testDf.drop(columns=[\"label\", \"difficulty\"])\n",
    "\n",
    "# Temporal features\n",
    "print(\"Extracting temporal features...\")\n",
    "trainTemporal = extract_temporal_features(trainDf, windowSizes=(2, 5, 10, 30, 100))\n",
    "testTemporal = extract_temporal_features(testDf, windowSizes=(2, 5, 10, 30, 100))\n",
    "\n",
    "trainXTemporal = pd.concat([trainXBase, trainTemporal], axis=1)\n",
    "testXTemporal = pd.concat([testXBase, testTemporal], axis=1)\n",
    "\n",
    "# For evaluation: ignore unknown test labels (-1)\n",
    "testValidMask = testYAll != -1\n",
    "testY = testYAll\n",
    "\n",
    "# Train baseline\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 1: TRAIN BASELINE MODEL\")\n",
    "print(\"=\" * 80)\n",
    "pipelineBaseline = train_multiclass_model(trainXBase, trainY)\n",
    "\n",
    "baselinePred = pipelineBaseline.predict(testXBase)\n",
    "baselineAcc = accuracy_score(testY[testValidMask], baselinePred[testValidMask])\n",
    "baselinePrec, baselineRec, baselineF1, _ = precision_recall_fscore_support(\n",
    "    testY[testValidMask], baselinePred[testValidMask], average=\"weighted\", zero_division=0\n",
    ")\n",
    "\n",
    "# Train temporal\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 2: TRAIN TEMPORAL MODEL\")\n",
    "print(\"=\" * 80)\n",
    "pipelineTemporal = train_multiclass_model(trainXTemporal, trainY)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 3: EVALUATION (TEMPORAL MODEL)\")\n",
    "print(\"=\" * 80)\n",
    "predictions, temporalMetrics = evaluate_multiclass(pipelineTemporal, testXTemporal, testY, validMask=testValidMask)\n",
    "\n",
    "temporalAcc = temporalMetrics[\"accuracy\"]\n",
    "temporalPrec = temporalMetrics[\"precision\"]\n",
    "temporalRec = temporalMetrics[\"recall\"]\n",
    "temporalF1 = temporalMetrics[\"f1score\"]\n",
    "\n",
    "# Compare summary\n",
    "comparisonDf = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Model\": \"Baseline\",\n",
    "            \"Accuracy\": baselineAcc,\n",
    "            \"Precision\": baselinePrec,\n",
    "            \"Recall\": baselineRec,\n",
    "            \"F1-Score\": baselineF1,\n",
    "        },\n",
    "        {\n",
    "            \"Model\": \"With Temporal Features\",\n",
    "            \"Accuracy\": temporalAcc,\n",
    "            \"Precision\": temporalPrec,\n",
    "            \"Recall\": temporalRec,\n",
    "            \"F1-Score\": temporalF1,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "print()\n",
    "print(\"Performance Comparison (valid labels only):\")\n",
    "print(comparisonDf.to_string(index=False))\n",
    "\n",
    "# Temporal feature importance (permutation)\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 4: TEMPORAL FEATURE IMPORTANCE (PERMUTATION)\")\n",
    "print(\"=\" * 80)\n",
    "temporalCols = list(trainTemporal.columns)\n",
    "temporalImportanceDf = compute_temporal_feature_importance(\n",
    "    pipelineTemporal, testXTemporal, testY, temporalCols, testValidMask, maxSamples=5000\n",
    ")\n",
    "if temporalImportanceDf is not None and len(temporalImportanceDf) > 0:\n",
    "    print(\"Top 10 temporal features (permutation importance):\")\n",
    "    print(temporalImportanceDf.head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"Temporal importance not computed or no temporal columns found\")\n",
    "\n",
    "# Multi-stage sequences (use temporal-augmented test df)\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 5: MULTI-STAGE ATTACK SEQUENCES\")\n",
    "print(\"=\" * 80)\n",
    "testDfWithTemporal = pd.concat([testDf, testTemporal], axis=1)\n",
    "attackSequences = detect_attack_sequences(testDfWithTemporal, predictions, windowSize=20)\n",
    "print(f\"Detected sequences: {len(attackSequences)}\")\n",
    "\n",
    "# MITRE mapping\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"PHASE 6: MITRE ATT&CK MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "ttpCounts, stageCounts = map_to_mitre_attack(testDf, predictions)\n",
    "print(f\"Unique TTPs detected: {len(ttpCounts)}\")\n",
    "\n",
    "# Save CSV outputs\n",
    "comparisonDf.to_csv(\"baseline_vs_temporal_comparison.csv\", index=False)\n",
    "if temporalImportanceDf is not None:\n",
    "    temporalImportanceDf.to_csv(\"temporal_feature_importance.csv\", index=False)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# EXPORT DATA FOR DASHBOARD\n",
    "# ------------------------------------------------------------------------\n",
    "classCounts = Counter(predictions)\n",
    "classCounts = {int(k): int(v) for k, v in classCounts.items()}\n",
    "\n",
    "# Timeline sample (up to 500 points)\n",
    "timelineData = []\n",
    "sampleIdx = np.linspace(0, len(predictions) - 1, min(500, len(predictions)), dtype=int)\n",
    "for i in sampleIdx:\n",
    "    timelineData.append(\n",
    "        {\n",
    "            \"index\": int(i),\n",
    "            \"prediction\": int(predictions[i]),\n",
    "            \"true_label\": int(testY[i]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Attack detection over time (windows of 100)\n",
    "attackTimeline = []\n",
    "for i in range(0, len(predictions), 100):\n",
    "    window = predictions[i : i + 100]\n",
    "    attackTimeline.append({\"window\": int(i), \"attacks\": int(np.sum(window > 0))})\n",
    "\n",
    "# Top temporal features for dashboard\n",
    "topTemporalFeatures = []\n",
    "if temporalImportanceDf is not None and len(temporalImportanceDf) > 0:\n",
    "    topDf = temporalImportanceDf.head(10)\n",
    "    topTemporalFeatures = [\n",
    "        {\"feature\": str(row[\"feature\"]), \"importance\": float(row[\"importance\"])} for _, row in topDf.iterrows()\n",
    "    ]\n",
    "\n",
    "# Format sequences for dashboard\n",
    "formattedSequences = []\n",
    "for seq in attackSequences[:10]:\n",
    "    tempIndicators = seq.get(\"temporal_indicators\", {})\n",
    "    tempIndicators = {str(k): int(v) for k, v in tempIndicators.items()}\n",
    "    formattedSequences.append(\n",
    "        {\n",
    "            \"pattern\": str(seq[\"pattern\"]),\n",
    "            \"severity\": str(seq[\"severity\"]),\n",
    "            \"attack_types\": \", \".join(seq[\"attack_types\"]),\n",
    "            \"window_start\": int(seq[\"window_start\"]),\n",
    "            \"window_end\": int(seq[\"window_end\"]),\n",
    "            \"temporal_indicators\": tempIndicators,\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Top TTPs\n",
    "topTtps = sorted(ttpCounts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "mitreData = [{\"ttp_id\": str(ttp), \"count\": int(count)} for ttp, count in topTtps]\n",
    "\n",
    "# Kill chain stages\n",
    "killChainData = [{\"stage\": str(stage), \"count\": int(count)} for stage, count in stageCounts.items()]\n",
    "\n",
    "dashboardData = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "        \"total_samples\": int(len(predictions)),\n",
    "        \"dataset\": \"NSL-KDD\",\n",
    "        \"note\": \"Metrics computed on valid labels only (unknown attacks mapped to -1 are excluded)\",\n",
    "    },\n",
    "    \"stats\": {\n",
    "        \"totalConnections\": int(len(predictions)),\n",
    "        \"attacksDetected\": int(np.sum(predictions > 0)),\n",
    "        \"attackRate\": float(np.mean(predictions > 0) * 100),\n",
    "        \"multiStageAttacks\": int(len(attackSequences)),\n",
    "        \"modelAccuracy\": float(temporalAcc * 100),\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"baseline\": {\n",
    "            \"accuracy\": float(baselineAcc * 100),\n",
    "            \"precision\": float(baselinePrec * 100),\n",
    "            \"recall\": float(baselineRec * 100),\n",
    "            \"f1score\": float(baselineF1 * 100),\n",
    "        },\n",
    "        \"temporal\": {\n",
    "            \"accuracy\": float(temporalAcc * 100),\n",
    "            \"precision\": float(temporalPrec * 100),\n",
    "            \"recall\": float(temporalRec * 100),\n",
    "            \"f1score\": float(temporalF1 * 100),\n",
    "        },\n",
    "    },\n",
    "    \"attackDistribution\": {\n",
    "        \"Normal\": int(classCounts.get(0, 0)),\n",
    "        \"DoS\": int(classCounts.get(1, 0)),\n",
    "        \"Probe\": int(classCounts.get(2, 0)),\n",
    "        \"R2L\": int(classCounts.get(3, 0)),\n",
    "        \"U2R\": int(classCounts.get(4, 0)),\n",
    "    },\n",
    "    \"attackTimeline\": attackTimeline,\n",
    "    \"timelineData\": timelineData,\n",
    "    \"temporalFeatures\": topTemporalFeatures,\n",
    "    \"attackSequences\": formattedSequences,\n",
    "    \"mitreAttack\": mitreData,\n",
    "    \"killChain\": killChainData,\n",
    "}\n",
    "\n",
    "with open(\"dashboard_data.json\", \"w\") as f:\n",
    "    json.dump(dashboardData, f, indent=2, cls=NumpyEncoder)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Saved:\")\n",
    "print(\"  baseline_vs_temporal_comparison.csv\")\n",
    "print(\"  temporal_feature_importance.csv (if computed)\")\n",
    "print(\"  dashboard_data.json\")\n",
    "print()\n",
    "print(\"Open your HTML dashboard via http.server and load the page in the browser\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ids_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
